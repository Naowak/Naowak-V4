# Déploiement d'une Solution RAG avec GraphRAG pour ReservoirPy

En tant que doctorant en intelligence artificielle et neuroscience, j'ai eu l'opportunité d'encadrer un stagiaire de M1, Virgile Boraud, pour un projet passionnant visant à déployer une solution [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) (Retrieval-Augmented Generation) avec [GraphRAG](https://github.com/microsoft/graphrag). L'objectif était de créer une documentation interactive basée sur des [LLM](https://fr.wikipedia.org/wiki/Grand_modèle_de_langage) (Large Language Models) pour une bibliothèque Python dédiée au développement de réseaux de neurones de type [Reservoir Computing](https://en.wikipedia.org/wiki/Reservoir_computing). Dans cet article, je vais vous raconter notre aventure et les défis que nous avons relevés.

## Motivations et équipe

[ReservoirPy](https://github.com/reservoirpy/reservoirpy) est une bibliothèque Python essentielle pour le développement de réseaux de neurones de type [Reservoir Computing](https://en.wikipedia.org/wiki/Reservoir_computing). Cette méthode de calcul est particulièrement efficace pour les tâches séquentielles, les séries chaotiques, et ce sans nécessiter beaucoup de données ni de puissance de calcul. Cependant, les utilisateurs de [ReservoirPy](https://github.com/reservoirpy/reservoirpy), souvent novices en développement informatique, rencontrent des difficultés à configurer les modèles et à comprendre les concepts sous-jacents. Il est crucial de fournir une assistance interactive pour aider ces utilisateurs à surmonter les défis techniques et à tirer pleinement parti de la bibliothèque. C'est pourquoi nous avons décidé de développer une solution basée sur des [LLM](https://fr.wikipedia.org/wiki/Grand_modèle_de_langage) et des Knowledge Graphs. Les [LLM](https://fr.wikipedia.org/wiki/Grand_modèle_de_langage) peuvent générer des réponses pertinentes et contextuelles, mais ils souffrent souvent de problèmes d'hallucination. En intégrant des Knowledge Graphs, nous pouvons améliorer la précision et la fiabilité des réponses. L'objectif est de créer une plateforme interactive qui puisse répondre aux questions des utilisateurs de manière précise et contextuelle, tout en leur fournissant des exemples de code et des explications détaillées.

![ReservoirPy sur Github](/articles/reservoirchat/reservoirpy.png)

Pour mener à bien ce projet, une équipe dévouée et compétente était essentielle. Virgile, notre stagiaire de M1, a été le développeur principal du projet. Il a travaillé sur l'intégration des différentes technologies et sur le développement de l'interface utilisateur. Loïc a joué un rôle crucial dans la mise en place de l'infrastructure et dans la configuration des services. Il a également contribué à la création de la configuration nginx. Paul et Xavier ont apporté leur expertise en neuroscience et en intelligence artificielle. Ils ont contribué à la définition des objectifs du projet et à l'évaluation des résultats. La collaboration entre les membres de l'équipe a été essentielle pour surmonter les défis techniques et pour assurer la réussite du projet. En tant qu'encadrant de Virgile, j'ai eu l'opportunité de guider Virgile tout au long du projet, en lui fournissant des conseils et des orientations pour l'aider à atteindre les objectifs fixés. 

## Choix de la solution

La mise en place de l'infrastructure a été une étape cruciale pour notre projet. Grâce à l'[Inria](https://inria.fr/fr), à l'équipe SED et à l'équipe derrière Plafrim, nous avons obtenu un accès exclusif à un nœud Sirocco de Plafrim, équipé de deux GPU V100 16Go. Cette machine puissante nous a permis de faire tourner [Codestral-22B](https://mistral.ai/fr/news/codestral/) - un [LLM](https://fr.wikipedia.org/wiki/Grand_modèle_de_langage) entrainé et distribué par [Mistral AI](https://mistral.ai/fr/) - en un temps record. Initialement, j'ai déployé les services avec **screen**, mais avec l'aide de Loïc, nous avons opté pour une solution plus robuste en utilisant **systemctl**. Loïc a également créé une configuration nginx pour exposer notre site web à l'adresse [http://chat.reservoirpy.inria.fr/](http://chat.reservoirpy.inria.fr/). Cette infrastructure solide nous a permis de surmonter les défis techniques initiaux et de poser les bases pour le développement de notre solution.

Pour améliorer les réponses des [LLM](https://fr.wikipedia.org/wiki/Grand_modèle_de_langage), nous avons d'abord exploré la méthode [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) (Retrieval-Augmented Generation). Cette méthode permet de récupérer des sections pertinentes de la documentation avant de générer une réponse, réduisant ainsi les hallucinations. Nous avons développé un simple [RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) en Python pour tester cette approche. Les résultats initiaux ont montré une nette amélioration dans la qualité des réponses générées, mais nous avons également appris l'importance de la qualité des données et de la pertinence des embeddings pour obtenir des résultats optimaux. Cette expérience nous a encouragés à aller plus loin en intégrant la méthode [GraphRAG](https://github.com/microsoft/graphrag).

![Graph de connaissance obtenu via GraphRAG](/articles/reservoirchat/graphrag.jpg)

[GraphRAG](https://github.com/microsoft/graphrag), développée par Microsoft, utilise des [LLM](https://fr.wikipedia.org/wiki/Grand_modèle_de_langage) pour extraire des concepts variés (personnes, lieux, objets, actions, etc.) à partir d'un ensemble de textes. Ces concepts sont ensuite associés à des résumés, liés entre eux, et regroupés par famille, créant ainsi un graphe de connaissances. Cette méthode permet d'améliorer la contextualisation et la précision des réponses générées. Virgile a lancé [GraphRAG](https://github.com/microsoft/graphrag) sur l'ensemble des documents collectés pour [ReservoirPy](https://github.com/reservoirpy/reservoirpy), et nous avons obtenu un graphe impressionnant. Les résultats obtenus avec [GraphRAG](https://github.com/microsoft/graphrag) ont été très prometteurs, avec une amélioration significative de la qualité des réponses générées. Cette étape a été essentielle pour atteindre nos objectifs et fournir une assistance interactive de haute qualité aux utilisateurs de [ReservoirPy](https://github.com/reservoirpy/reservoirpy).

## Implémentation et Évaluation

Le développement de [ReservoirChat](https://github.com/VirgileBoraud/ReservoirChat) a été une étape cruciale de notre projet. L'interface utilisateur, conçue avec la bibliothèque Panel, offre une expérience interactive et intuitive. Les utilisateurs peuvent poser des questions et recevoir des réponses contextuelles et précises, grâce à l'intégration de plusieurs fonctionnalités clés. [ReservoirChat](https://github.com/VirgileBoraud/ReservoirChat) utilise des [LLM](https://fr.wikipedia.org/wiki/Grand_modèle_de_langage) pour générer des réponses, améliorées par des Knowledge Graphs pour une meilleure précision. De plus, la plateforme est capable de fournir des exemples de code et des explications détaillées, rendant l'assistance encore plus complète. Cependant, l'intégration de ces différentes technologies a présenté plusieurs défis techniques, notamment la compatibilité entre les services et la gestion des ressources que nous avons su résoudre avec succès.

![Interface de ReservoirChat](/articles/reservoirchat/reservoirchat.png)

Pour évaluer les performances de [ReservoirChat](https://github.com/VirgileBoraud/ReservoirChat), nous avons adopté une méthodologie de benchmarking. Cette approche consiste à soumettre [ReservoirChat](https://github.com/VirgileBoraud/ReservoirChat) à un ensemble de Questions à Choix Multiple (QCM) préparé à l'avance, et à le comparer avec les autres modèles concurrents. Nous avons comparé ses performances avec [ChatGPT-4o](https://chatgpt.com), [Llama3](https://ai.meta.com/blog/meta-llama-3/), [Codestral](https://mistral.ai/fr/news/codestral/), et [NotebookLM](https://notebooklm.google). Les résultats ont montré que [ReservoirChat](https://github.com/VirgileBoraud/ReservoirChat) offre des performances compétitives et fiables, avec une amélioration significative de la qualité des réponses générées grâce à l'intégration des Knowledge Graphs. Cette comparaison a également mis en lumière les forces et les faiblesses de notre solution, nous permettant d'identifier des axes d'amélioration pour l'avenir.

## Conclusion et Perspectives

En conclusion, notre projet de développement de [ReservoirChat](https://github.com/VirgileBoraud/ReservoirChat) a permis d'atteindre des résultats significatifs et de fournir une assistance interactive aux utilisateurs de [ReservoirPy](https://github.com/reservoirpy/reservoirpy). L'intégration des Knowledge Graphs a considérablement amélioré la précision et la pertinence des réponses générées, montrant l'importance d'utilisation de base de données structurées pour améliorer les performances des [LLM](https://fr.wikipedia.org/wiki/Grand_modèle_de_langage).

[ReservoirChat](https://github.com/VirgileBoraud/ReservoirChat) offre une nouvelle dimension à l'assistance interactive, permettant aux utilisateurs de mieux comprendre et utiliser la bibliothèque [ReservoirPy](https://github.com/reservoirpy/reservoirpy). Ce projet ouvre la voie à de nouvelles approches et méthodes pour rendre les bibliothèques de développement informatique plus accessibles et interactives.

Je remercie Virgile, Loïc, Paul, Xavier et toute l'équipe pour leur dévouement et leur expertise tout au long du projet. Cette expérience a été enrichissante et stimulante.

##### Yannis Bendi-Ouis