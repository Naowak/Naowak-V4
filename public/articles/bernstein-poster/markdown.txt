# Poster scientifique sur les Recurrent Attention Network pour la conférence Bernstein 2024

Pour la conférence Bernstein 2024, j'ai préparé un poster sur mes travaux de Thèse : les Recurrent Attention Networks. Ces travaux proposent une nouvelle architecture de réseaux de neuronne récurrent pour le traitement de séquences et s'inspirent des Transformers et du Reservoir Computing pour modéliser un réservoir scalable possèdant des unités plus complexe : des unités d'attention.

> Avec le récent succès des Transformers dans le domaine du traitement automatique du langage naturel, il est devenu évident que nous sommes capables de créer des architectures de réseaux de neurones complexes et performantes qui peuvent extraire, combiner et transformer des informations pour atteindre un objectif spécifique, comme prédire le mot suivant dans un texte. Cependant, pour fonctionner, les Transformers doivent recevoir un contexte de tous les N éléments de la séquence passée (N = 2k à 32k jetons) à chaque étape temporelle, ce qui fait croître le nombre de calculs de manière quadratique (N^2) avec la longueur de la séquence. Cette "mémoire parfaite" d'un contexte de milliers d'éléments est équivalente, en neurosciences, à une mémoire de travail biologiquement irréaliste. Afin d'être plus biologiquement plausible, nous proposons de réduire drastiquement la taille de ce contexte (permettant des calculs plus locaux et moins coûteux), tout en introduisant des récurrences permettant l'utilisation d'une mémoire de travail à plus long terme.
>
> Une manière efficace et biologiquement plausible d'avoir des récurrences est de se placer dans le paradigme du Reservoir Computing, où les connexions récurrentes sont aléatoires et la couche de sortie sert de lecture entraînée par régression linéaire (ou équivalence). Pour la neuroscience computationnelle, le Reservoir Computing est une approche très intéressante pour modéliser des calculs non linéaires de haute dimension avec une faible complexité computationnelle.
>
> Contrairement à d'autres approches qui cherchent à modifier les calculs derrière l'attention (RWKV, RetNet), nous proposons de combiner ces deux modèles afin de bénéficier à la fois des calculs attentionnels des Transformers et de la mémoire de travail du Reservoir Computing. De la même manière que les calculs dendritiques nous montrent que les neurones sont capables de calculs plus complexes, nous pensons qu'il pourrait être intéressant de remplacer les neurones (unité tanh) présents dans un Réservoir par des unités plus complexes, comme les blocs d'attention présents dans les Transformers, même pour une courte période.
> 
> Nous évaluons deux nouvelles architectures sur un ensemble de tâches séquentielles (par exemple, JapaneseVowels, Cross-Situational Learning) : l'une contenant des unités équivalentes aux blocs d'attention des transformers, et l'autre décomposant le bloc d'attention en deux types d'unités : une unité basée sur le mécanisme QKV (Query-Key-Value) et une autre unité de "mémoires" (Feed-Forward), permettant ainsi de modéliser différentes structures cérébrales (cortex, hippocampe).

![Poster sur les Recurrent Attention Network pour Bernstein 2024](/articles/bernstein-poster/final-poster-a0.pdf)

Vous pouvez retrouver les informations de cette conférence ici : [Bernstein Conference Website](https://bernstein-network.de/bernstein-conference/).

##### Yannis Bendi-Ouis